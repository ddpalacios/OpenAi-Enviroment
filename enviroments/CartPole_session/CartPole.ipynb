{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAction_space --> provides specification for allowed actions in the enviroment\\n\\nObservation_space --> Specifies observation provided by the enviroment\\n\\nreset() --> Resets the enviroment to its inital state, returining the initial observation vector\\n\\nstep() --> Allows agent to give the action and returns the information about the outcome of the action\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# We will only discuss the fundementals of this library. I will try to be as detailed as I can be, however,\n",
    "# I recommend getting the book \"Deep Reinforcement Learning by Maxin Lapan\" in order to fill in any gaps\n",
    "\n",
    "'''\n",
    "In Reinforcement Learning, we have general entities and their communication with one and other\n",
    "\n",
    "We use GYM to avoid the complication game enviroments when working with RL\n",
    "\n",
    "We have:\n",
    "\n",
    "#AGENT --(performes some action)-> #ENVIROMENT --> (Gives some reward to agent based on action and observes agents)\n",
    "\n",
    "I wont go over in too much details in each of these entities so you are just going to have to follow along\n",
    "with the code and any important details that you need to know I will then mention. \n",
    "\n",
    "'''\n",
    "\n",
    "# The enviroment is represented in Gym by the Env class which has the following memebers\n",
    "\n",
    "'''\n",
    "Action_space --> provides specification for allowed actions in the enviroment\n",
    "\n",
    "Observation_space --> Specifies observation provided by the enviroment\n",
    "\n",
    "reset() --> Resets the enviroment to its inital state, returining the initial observation vector\n",
    "\n",
    "step() --> Allows agent to give the action and returns the information about the outcome of the action\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets open our first enviroment --> CartPole-v0\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() # Must always do \n",
    "# env.render() # To display on screen\n",
    "\n",
    "# The observation of this enviroment is four float numbers containing information about the x-cord\n",
    "# of the sticks center of mass. ots speed. its angle to the platform, and its angular speed\n",
    "\n",
    "# Enough talkking, lets start building ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes done in 1 steps, total reward 0.00 \n"
     ]
    }
   ],
   "source": [
    "# For demonstration we will give the agent some random actions\n",
    "# This is just for an agents interation for their enviroment\n",
    "total_reward = 0.0\n",
    "total_steps  = 0\n",
    "while True:\n",
    "    action = env.action_space.sample() # Random movement to express enviroment\n",
    "    obs, reward, done , _ = env.step(action)\n",
    "    env.render()\n",
    "    # observation --> [Mass, Speed, Angle (to the platform), Angular Speed]\n",
    "    total_reward+= reward\n",
    "    total_steps +=1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Episodes done in %d steps, total reward %.2f \"%(total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to next scipt --> BuildingRLmodel.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
